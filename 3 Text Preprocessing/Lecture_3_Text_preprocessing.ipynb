{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b414bc5a-c0cf-4d4e-bbeb-de8dd1109f0c",
   "metadata": {},
   "source": [
    "## Text Preprocessing Basics \n",
    "\n",
    "- Lower Casing\n",
    "- Removing HTML Tags\n",
    "- Removing URLs\n",
    "- Removing Punctuation\n",
    "- Chat words treatment\n",
    "- Spelling Correction\n",
    "- Removing Stopwords\n",
    "- Handling Emojis\n",
    "- Tokenization\n",
    "- Stemming\n",
    "- Lemmatization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0f3039d-110e-4d5f-826b-34acce2de533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing numpy and pandas\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ad7e976-540a-4655-a730-f9a16ee26dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data set Link \n",
    "# https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d32c1799-90d3-4005-b26f-eda2567fabd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the CSV file and storing the data in a DataFrame 'df'\n",
    "df = pd.read_csv(\"../archive/IMDB Dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae6f4e00-fe78-48af-a6dd-238da2f57e33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>I thought this movie did a down right good job...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>Bad plot, bad dialogue, bad acting, idiotic di...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>I am a Catholic taught in parochial elementary...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>I'm going to have to disagree with the previou...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>No one expects the Star Trek movies to be high...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment\n",
       "0      One of the other reviewers has mentioned that ...  positive\n",
       "1      A wonderful little production. <br /><br />The...  positive\n",
       "2      I thought this was a wonderful way to spend ti...  positive\n",
       "3      Basically there's a family where a little boy ...  negative\n",
       "4      Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
       "...                                                  ...       ...\n",
       "49995  I thought this movie did a down right good job...  positive\n",
       "49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative\n",
       "49997  I am a Catholic taught in parochial elementary...  negative\n",
       "49998  I'm going to have to disagree with the previou...  negative\n",
       "49999  No one expects the Star Trek movies to be high...  negative\n",
       "\n",
       "[50000 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30114a7e-ecb6-41ae-aa98-8d0d0501488f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a58afb1f-a481-47e4-8c00-3a3118114bc0",
   "metadata": {},
   "source": [
    "## Lowercasing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc90a78a-cf2c-403f-810c-f84b1b761872",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"basically there's a family where a little boy (jake) thinks there's a zombie in his closet & his parents are fighting all the time.<br /><br />this movie is slower than a soap opera... and suddenly, jake decides to become rambo and kill the zombie.<br /><br />ok, first of all when you're going to make a film you must decide if its a thriller or a drama! as a drama the movie is watchable. parents are divorcing & arguing like in real life. and then we have jake with his closet which totally ruins all the film! i expected to see a boogeyman similar movie, and instead i watched a drama with some meaningless thriller spots.<br /><br />3 out of 10 just for the well playing parents & descent dialogs. as for the shots with jake: just ignore them.\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accessing the 'review' column of the DataFrame 'df' and converting the text \n",
    "# in the fourth row (index 3) to lowercase\n",
    "\n",
    "lowercase_review = df['review'][3].lower()\n",
    "lowercase_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37183658-4e4f-43c5-903a-8eacc7804b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting all the text in the 'review' column of the DataFrame 'df' to lowercase\n",
    "df['review'] = df['review'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70f7b30a-c09b-458b-b819-0b8607bc2321",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1547</th>\n",
       "      <td>the comments already left for this show are wa...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46095</th>\n",
       "      <td>i saw this film at temple university. i cannot...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22827</th>\n",
       "      <td>simple, meaningful and delivers an emotional p...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31455</th>\n",
       "      <td>this film is terrible. the story concerns a wo...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24724</th>\n",
       "      <td>since i am a fan of natalie portman, i had to ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment\n",
       "1547   the comments already left for this show are wa...  negative\n",
       "46095  i saw this film at temple university. i cannot...  negative\n",
       "22827  simple, meaningful and delivers an emotional p...  positive\n",
       "31455  this film is terrible. the story concerns a wo...  negative\n",
       "24724  since i am a fan of natalie portman, i had to ...  positive"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46daa5e2-8485-455f-ae1b-1af9ec2f6e41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "33cf421d-b6aa-4519-b11a-ac474a51dc0e",
   "metadata": {},
   "source": [
    "## Remove HTML Tags "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2dce2ee-3110-468b-9338-5137c0b76dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_html_tags(text):\n",
    "    # Define the regular expression pattern to match HTML tags\n",
    "    pattern = re.compile('<.*?>')\n",
    "    \n",
    "    # Use the 'sub' method to replace all occurrences of HTML tags with \n",
    "    # an empty string\n",
    "    # This effectively removes all HTML tags from the 'text' input\n",
    "    return pattern.sub(r'', text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0892da20-7cd5-4b4f-b792-308992196f0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "basically there's a family where a little boy (jake) thinks there's a zombie in his closet & his par\n"
     ]
    }
   ],
   "source": [
    "# Removing HTML tags from the 'review' text in the fourth row (index 3) of the DataFrame 'df'\n",
    "cleaned_review = remove_html_tags(df['review'][3])\n",
    "print(cleaned_review[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12ab5be5-5347-4759-88fb-54af1560a72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the 'remove_html_tags()' function to the 'review' column \n",
    "# This will remove HTML tags from all the reviews in the 'review' column\n",
    "df['review'] = df['review'].apply(remove_html_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "847c3f85-c753-486c-a65a-f0afb3ebcede",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12161</th>\n",
       "      <td>ok, so, chuck norris somehow found a way to ge...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28835</th>\n",
       "      <td>faithful to the work of pearl s. buck whose ye...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32826</th>\n",
       "      <td>this excruciatingly boring and unfunny movie m...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33854</th>\n",
       "      <td>well another shootem up. typical run around fi...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44569</th>\n",
       "      <td>poor michael madsen; he must be kicking himsel...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment\n",
       "12161  ok, so, chuck norris somehow found a way to ge...  negative\n",
       "28835  faithful to the work of pearl s. buck whose ye...  positive\n",
       "32826  this excruciatingly boring and unfunny movie m...  negative\n",
       "33854  well another shootem up. typical run around fi...  negative\n",
       "44569  poor michael madsen; he must be kicking himsel...  negative"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c860e71-3836-48d2-b9c2-cf443c23328e",
   "metadata": {},
   "source": [
    "## Remove URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3819fe0-8a5f-4163-9177-8d15479ac23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_url(text):\n",
    "    # Define the regular expression pattern to match URLs\n",
    "    pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    \n",
    "    # Use the 'sub' method to replace all occurrences of URLs with an empty string\n",
    "    # This effectively removes all URLs from the 'text' input\n",
    "    return pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "461ecf81-cb47-4d4e-9332-3a9c1a13b073",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = 'Check out my notebook https://www.kaggle.com/campusx/notebook'\n",
    "text2 = 'Check out my notebook http://www.kaggle.com/campusx/notebook'\n",
    "text3 = 'Check out my notebook www.kaggle.com'\n",
    "text4 = 'Check out my notebook https://www.kaggle.com/campusx/notebook and also www.google.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "989669e5-ac38-49d7-a9be-108c32a91bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check out my notebook \n",
      "Check out my notebook \n",
      "Check out my notebook \n",
      "Check out my notebook  and also \n"
     ]
    }
   ],
   "source": [
    "for text in [text1,text2,text3,text4]:\n",
    "    print(remove_url(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa53fa8a-2e03-4533-b534-0f66acb06e0c",
   "metadata": {},
   "source": [
    "## Remove Punctuation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a453a997-f5d0-4ba0-89f9-d7f3d299ee28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "import time\n",
    "\n",
    "# The 'string.punctuation' attribute contains all punctuation characters\n",
    "# It includes symbols like !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
    "punctuation_chars = string.punctuation\n",
    "punctuation_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "59534751-b80e-4f13-ba69-b03b2396c83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude = string.punctuation\n",
    "\n",
    "def remove_punc(text):\n",
    "    # Iterate through each character in 'exclude'\n",
    "    for char in exclude:\n",
    "        # Replace the current character with an empty string in 'text'\n",
    "        text = text.replace(char, '')\n",
    "    \n",
    "    # Return the 'text' with all punctuation characters removed\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "30a60d70-8c75-4b52-aa64-d9d5d96e54a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'string . with ! #  punctuation ? '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "06743559-2ed7-489d-a57d-029696601188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "string  with    punctuation  \n",
      "0.00018095970153808594\n"
     ]
    }
   ],
   "source": [
    "# Record the start time before calling the function\n",
    "start = time.time()\n",
    "# Call the 'remove_punc' function to remove punctuation from the 'text'\n",
    "print(remove_punc(text))\n",
    "# Calculate the time taken to execute the 'remove_punc' function\n",
    "time1 = time.time() - start\n",
    "# Print the time taken in seconds\n",
    "print(time1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c6e079b4-e912-4862-a0d3-686129ef3bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    " # This approach is very slow and inefficient when the dataset is large "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "97346c5f-d48f-4abd-a2d3-cda50c6ff9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punc1(text):\n",
    "    return text.translate(str.maketrans('','',exclude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "71b446a5-d37d-47da-b08e-c1b1e48fab1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "string  with    punctuation  \n",
      "0.00034427642822265625\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "print(remove_punc1(text))\n",
    "time2 = time.time() - start\n",
    "print(time2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8c0de26f-f705-434c-973d-a2c0c69df58b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.525623268698061"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time1/time2  # 18 times faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f0e4d3c9-4bd4-4df3-a56b-80b5fee69cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the 'remove_punc1' function to the 'review' column of the DataFrame 'df'\n",
    "# This will remove punctuation from all the reviews in the 'review' column\n",
    "df['review'] = df['review'].apply(remove_punc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "73ab0267-8346-4980-87db-db08d79e9019",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6820</th>\n",
       "      <td>am i the only one to notice that the realism o...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8758</th>\n",
       "      <td>scoop is also the name of a latethirties evely...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34339</th>\n",
       "      <td>in sri lanka a country divided by religion and...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment\n",
       "6820   am i the only one to notice that the realism o...  negative\n",
       "8758   scoop is also the name of a latethirties evely...  positive\n",
       "34339  in sri lanka a country divided by religion and...  positive"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3a92c5-4670-44d5-9e60-8065c695aeb5",
   "metadata": {},
   "source": [
    "## Chat word Treatment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eb883fe4-b06c-44fd-90e8-5005ea2dfe82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slang words \n",
    "# https://github.com/rishabhverma17/sms_slang_translator/blob/master/slang.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "265690e6-db72-4e9e-bb59-fe64a5f879c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "url = 'https://raw.githubusercontent.com/rishabhverma17/sms_slang_translator/master/slang.txt'\n",
    "page = requests.get(url)\n",
    "chat_words = page.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4f39ec53-221d-4707-8ce5-176e95047fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_words = chat_words.split('\\n')\n",
    "chat_words_dict = {}\n",
    "for line in chat_words:\n",
    "    key_n_val = line.split('=')\n",
    "    try:\n",
    "        chat_words_dict[key_n_val[0]] =  key_n_val[1]\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "097d0d44-da84-42a2-bdb0-f26259dccecd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AFAIK': 'As Far As I Know',\n",
       " 'AFK': 'Away From Keyboard',\n",
       " 'ASAP': 'As Soon As Possible',\n",
       " 'ATK': 'At The Keyboard',\n",
       " 'ATM': 'At The Moment',\n",
       " 'A3': 'Anytime, Anywhere, Anyplace',\n",
       " 'BAK': 'Back At Keyboard',\n",
       " 'BBL': 'Be Back Later',\n",
       " 'BBS': 'Be Back Soon',\n",
       " 'BFN': 'Bye For Now',\n",
       " 'B4N': 'Bye For Now',\n",
       " 'BRB': 'Be Right Back',\n",
       " 'BRT': 'Be Right There',\n",
       " 'BTW': 'By The Way',\n",
       " 'B4': 'Before',\n",
       " 'CU': 'See You',\n",
       " 'CUL8R': 'See You Later',\n",
       " 'CYA': 'See You',\n",
       " 'FAQ': 'Frequently Asked Questions',\n",
       " 'FC': 'Fingers Crossed',\n",
       " 'FWIW': \"For What It's Worth\",\n",
       " 'FYI': 'For Your Information',\n",
       " 'GAL': 'Get A Life',\n",
       " 'GG': 'Good Game',\n",
       " 'GN': 'Good Night',\n",
       " 'GMTA': 'Great Minds Think Alike',\n",
       " 'GR8': 'Great!',\n",
       " 'G9': 'Genius',\n",
       " 'IC': 'I See',\n",
       " 'ICQ': 'I Seek you (also a chat program)',\n",
       " 'ILU': 'ILU: I Love You',\n",
       " 'IMHO': 'In My Honest/Humble Opinion',\n",
       " 'IMO': 'In My Opinion',\n",
       " 'IOW': 'In Other Words',\n",
       " 'IRL': 'In Real Life',\n",
       " 'KISS': 'Keep It Simple, Stupid',\n",
       " 'LDR': 'Long Distance Relationship',\n",
       " 'LMAO': 'Laugh My A.. Off',\n",
       " 'LOL': 'Laughing Out Loud',\n",
       " 'LTNS': 'Long Time No See',\n",
       " 'L8R': 'Later',\n",
       " 'MTE': 'My Thoughts Exactly',\n",
       " 'M8': 'Mate',\n",
       " 'NRN': 'No Reply Necessary',\n",
       " 'OIC': 'Oh I See',\n",
       " 'PITA': 'Pain In The A..',\n",
       " 'PRT': 'Party',\n",
       " 'PRW': 'Parents Are Watching',\n",
       " 'ROFL': 'Rolling On The Floor Laughing',\n",
       " 'ROFLOL': 'Rolling On The Floor Laughing Out Loud',\n",
       " 'ROTFLMAO': 'Rolling On The Floor Laughing My A.. Off',\n",
       " 'SK8': 'Skate',\n",
       " 'STATS': 'Your sex and age',\n",
       " 'ASL': 'Age, Sex, Location',\n",
       " 'THX': 'Thank You',\n",
       " 'TTFN': 'Ta-Ta For Now!',\n",
       " 'TTYL': 'Talk To You Later',\n",
       " 'U': 'You',\n",
       " 'U2': 'You Too',\n",
       " 'U4E': 'Yours For Ever',\n",
       " 'WB': 'Welcome Back',\n",
       " 'WTF': 'What The F...',\n",
       " 'WTG': 'Way To Go!',\n",
       " 'WUF': 'Where Are You From?',\n",
       " 'W8': 'Wait...',\n",
       " '7K': 'Sick:-D Laugher'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_words_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "968451af-8f4f-404e-8b7d-bec922218605",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_conversation(text):\n",
    "    new_text = []\n",
    "    for w in text.split():\n",
    "        if w.upper() in chat_words_dict:\n",
    "            new_text.append(chat_words_dict[w.upper()])\n",
    "        else:\n",
    "            new_text.append(w)\n",
    "    return \" \".join(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3c191ae0-e83f-4bd3-af59-a20b27495e9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In My Honest/Humble Opinion he is good'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_conversation('IMHO he is good')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "36e62445-3d3f-47f8-9cc9-d31c15cb14e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'For Your Information he is good, hes Genius'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_conversation('FYI he is good, hes G9')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc769cb-6b2c-4803-a557-eb763b992716",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2cf86f93-dacc-4633-a17a-3819e7e1a8c7",
   "metadata": {},
   "source": [
    "## Spelling Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "75ec3643-854c-42b0-90dd-0039cba69562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "certain conditions during several generations are modified in the same manner.\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "# The text with incorrect spelling\n",
    "incorrect = 'ceertain conditionas duriing seveal geenarations aree moodified in the samme maner.'\n",
    "\n",
    "# Create a TextBlob object with the incorrect text\n",
    "textBlb = TextBlob(incorrect)\n",
    "\n",
    "# Use the 'correct()' method to correct spelling mistakes in the text\n",
    "# The 'string' attribute will retrieve the corrected text as a string\n",
    "corrected_text = textBlb.correct().string\n",
    "\n",
    "# Print the corrected text\n",
    "print(corrected_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95c3d17-d630-4e3a-888c-00ca45ebbdb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177e1865-763a-453f-95d9-928437f724cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1af4b09a-473e-42f3-b92f-0479c68f8f8e",
   "metadata": {},
   "source": [
    "## Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "36c9979c-883d-4e3b-a0e3-6845aca74c3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Accessing the list of stopwords in the English language\n",
    "stop_words_english = stopwords.words('english')\n",
    "\n",
    "# Print the list of stopwords - printing only 20 \n",
    "print(stop_words_english[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e1938520-12a5-4037-abd2-a48cee758082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopwords.words('spanish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "948270e9-4a56-4ee1-bb39-ca2778003131",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    # Create an empty list to store non-stopwords\n",
    "    new_text = []\n",
    "    # Split the input 'text' into individual words and iterate through each word\n",
    "    for word in text.split():\n",
    "        # Check if the word is a stopword (found in the 'stopwords.words('english')' list)\n",
    "        if word in stopwords.words('english'):\n",
    "            # If it is a stopword, append an empty string to 'new_text'\n",
    "            # This effectively removes stopwords from the text\n",
    "            new_text.append('')\n",
    "        else:\n",
    "            # If it is not a stopword, append the word to 'new_text'\n",
    "            new_text.append(word)\n",
    "    # Create a copy of 'new_text' and clear the original 'new_text' list\n",
    "    x = new_text[:]\n",
    "    new_text.clear()\n",
    "    # Join the words in 'x' back into a single string using spaces and return it\n",
    "    return \" \".join(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cb8b5aca-cd8e-44aa-862c-619416aeae54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As  gentle breeze rustled   leaves,  vibrant colors   autumn foliage danced  harmony, creating  breathtaking tapestry  nature's artistry  captivated  hearts    beheld it.\n"
     ]
    }
   ],
   "source": [
    "text = \"As the gentle breeze rustled through the leaves, the vibrant colors of the autumn foliage danced in harmony, creating a breathtaking tapestry of nature's artistry that captivated the hearts of all who beheld it.\"\n",
    "print(remove_stopwords(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c6bb9dd5-df44-4f7e-bb13-f1d827a159ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    one    reviewers  mentioned   watching  1 oz e...\n",
       "1     wonderful little production  filming techniqu...\n",
       "2     thought    wonderful way  spend time    hot s...\n",
       "3    basically theres  family   little boy jake thi...\n",
       "4    petter matteis love   time  money   visually s...\n",
       "5    probably  alltime favorite movie  story  selfl...\n",
       "6     sure would like  see  resurrection    dated s...\n",
       "7     show   amazing fresh innovative idea   70s   ...\n",
       "8    encouraged   positive comments   film     look...\n",
       "9      like original gut wrenching laughter   like ...\n",
       "Name: review, dtype: object"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review'][:10].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c539342-ab8a-420d-b2a1-afaaa4464909",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1461544d-e6af-4ff8-9dc4-7897dc9f1778",
   "metadata": {},
   "source": [
    "## Handling emojis "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501b5990-f42e-4b8e-838b-266205fb23f8",
   "metadata": {},
   "source": [
    "### Removing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3933c109-b1a5-4bec-92bb-9bf4f021d2f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " hello \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def remove_emoji(text):\n",
    "    # Define the regular expression pattern to match emojis\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags\n",
    "                           u\"\\U00002702-\\U000027B0\"  # other miscellaneous symbols\n",
    "                               u\"\\U000024C2-\\U0001F251\"  # enclosed characters\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    # Use 're.sub()' to replace all occurrences of emojis with an empty string\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "# Test the function with an example text containing emojis\n",
    "result_text = remove_emoji(\"ðŸ˜„ðŸ˜„ðŸ˜„ðŸ˜„ hello ðŸ˜„ðŸ˜„ðŸ˜„ðŸ˜„\")\n",
    "print(result_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b08bf7-91d0-4dab-8a53-d7d26ec40467",
   "metadata": {},
   "source": [
    "### Replacing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fb00781f-fb4a-4657-8f15-f51f434ea515",
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5ae0ee9d-8fe4-490b-a244-941a82cbbaec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "':grinning_face_with_smiling_eyes::grinning_face_with_smiling_eyes::grinning_face_with_smiling_eyes::grinning_face_with_smiling_eyes: hello :grinning_face_with_smiling_eyes::grinning_face_with_smiling_eyes::grinning_face_with_smiling_eyes::grinning_face_with_smiling_eyes:'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emoji.demojize(\"ðŸ˜„ðŸ˜„ðŸ˜„ðŸ˜„ hello ðŸ˜„ðŸ˜„ðŸ˜„ðŸ˜„\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36197a47-f1cd-406b-a802-04705980526a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c766be43-0360-4947-98a7-85ebee76ec7c",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d2031c-a568-4d80-b41b-57d134b301dc",
   "metadata": {},
   "source": [
    "Tokenization in NLP preprocessing is the process of breaking down a text or a sentence into smaller units called tokens. These tokens are typically words or subwords, and tokenization is a fundamental step in preparing text data for various NLP tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543b5384-da4c-476a-b88e-a5789e421582",
   "metadata": {},
   "source": [
    "### 1. Using the split function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0a5252c9-182b-46e1-a1a4-a46f12cc3811",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'going', 'to', 'delhi']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word tokenization \n",
    "sent1 = 'I am going to delhi'\n",
    "sent1.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d6ad25fe-b4cf-42c0-9b27-0282a73313a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I am going to delhi',\n",
       " ' I will stay there for 2 days',\n",
       " ' Lets hope the trip will be good']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sentence tokenization \n",
    "sent2 = 'I am going to delhi. I will stay there for 2 days. Lets hope the trip will be good'\n",
    "sent2.split('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e69e3d25-9963-4ea1-a04a-a5abdb9330ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'going', 'to', 'delhi!']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Problem with split function \n",
    "sent3 = 'I am going to delhi!'\n",
    "sent3.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6618825e-ceeb-4247-a69f-3ae31a759dad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Where do you think I should go? I have 3 day holiday']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent4 = 'Where do you think I should go? I have 3 day holiday'\n",
    "sent4.split('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828fce08-f273-42a3-b618-8d4f50d93474",
   "metadata": {},
   "source": [
    "### 2. Use Regualr expressions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "168af045-2b11-4cc7-806b-26eafbf99561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'going', 'to', 'delhi']\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "# The sentence to tokenize\n",
    "sent3 = 'I am going to delhi!'\n",
    "\n",
    "# Use 're.findall()' to find all word and apostrophe tokens in the sentence\n",
    "# '\\w' matches any word character (letters, digits, or underscore), and \"'\" matches apostrophes\n",
    "# '+' matches one or more occurrences of the pattern (one or more word characters and/or apostrophes)\n",
    "# The result will be a list of all tokens in the sentence\n",
    "tokens = re.findall(\"[\\w']+\", sent3)\n",
    "\n",
    "# Print the list of tokens\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "155fd725-edee-48de-9a45-1e131db2f84d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lorem ipsum is simply dummy text of the printing and typesetting industry',\n",
       " \"\\n Lorem Ipsum has been the industry's standard dummy text ever since the 1500s,\\n when an unknown printer took a gallery of type and scrambled it to make a type specimen book\",\n",
       " '']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '''Lorem ipsum is simply dummy text of the printing and typesetting industry?\n",
    " Lorem Ipsum has been the industry's standard dummy text ever since the 1500s,\n",
    " when an unknown printer took a gallery of type and scrambled it to make a type specimen book.'''\n",
    "\n",
    "sentences = re.compile('[.!?]').split(text)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10f5bf6-5a9f-4155-9d75-86f0f00d45a0",
   "metadata": {},
   "source": [
    "### 3. NLTK "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1f79a991-6ff9-412f-a7d7-ce41d9a945fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize,sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "99d6b3ea-5bf6-4e8a-aa4a-71f160d22e73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'going', 'to', 'visit', 'delhi', '!']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent1 = 'I am going to visit delhi!'\n",
    "word_tokenize(sent1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ca2a281f-ffe2-48bf-9ce3-89a3fab038f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lorem ipsum is simply dummy text of the printing and typesetting industry?',\n",
       " \"Lorem Ipsum has been the industry's standard dummy text ever since the 1500s,\\n when an unknown printer took a gallery of type and scrambled it to make a type specimen book.\"]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '''Lorem ipsum is simply dummy text of the printing and typesetting industry?\n",
    " Lorem Ipsum has been the industry's standard dummy text ever since the 1500s,\n",
    " when an unknown printer took a gallery of type and scrambled it to make a type specimen book.'''\n",
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d0075414-0c0b-4fb6-bd69-58170cb9b340",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent5 = 'I have a Ph.D in A.I'\n",
    "sent6 = \"We're here to help! mail us at nks@gmail.com\"\n",
    "sent7 = 'A 5km ride cost $10.58'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "79cc5179-a068-45a5-8973-525d0bdf126a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'have', 'a', 'Ph.D', 'in', 'A.I']\n",
      "['We', \"'re\", 'here', 'to', 'help', '!', 'mail', 'us', 'at', 'nks', '@', 'gmail.com']\n",
      "['A', '5km', 'ride', 'cost', '$', '10.58']\n"
     ]
    }
   ],
   "source": [
    "for sent in [sent5,sent6,sent7]:\n",
    "    print(word_tokenize(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c580bebc-1ad9-4dd5-9d92-eddd15b0149e",
   "metadata": {},
   "source": [
    "### 4. Spacy (BEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "910654d1-2ce1-43c1-8e34-1aa76fd5c502",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "dcaa6e24-1bff-4b95-9038-215af825cb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = nlp(sent5)\n",
    "doc2 = nlp(sent6)\n",
    "doc3 = nlp(sent7)\n",
    "# doc4 = nlp(sent8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "eae8568a-08f0-4aa9-805b-4fadba0f284f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "have\n",
      "a\n",
      "Ph\n",
      ".\n",
      "D\n",
      "in\n",
      "A.I\n"
     ]
    }
   ],
   "source": [
    "for token in doc1:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd5d71f-5746-4b3f-b5f0-809e26e87324",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5321e92f-85f5-4bc2-bfe2-d63c08ec7eb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a6f2f7-2e8b-4f82-8ced-555ba23eb651",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb440055-2f86-43a3-8722-accfc7426a91",
   "metadata": {},
   "source": [
    "## Stemming \n",
    "\n",
    "In grammar, inflection is the modificatoin of a word to express different grammatical categories such as tense,case,voice,aspect,person,number,gender,and mood.\n",
    "\n",
    "eg. Walk - walk,walking,walked,walker,walks\n",
    "\n",
    "_Stemming_ is the process of reducing inflection in words to their root forms such as mapping a group of words to the same stem even if the stem itself is not a valid word in the Language.\n",
    "\n",
    "- its mostly used in information retreival systems (google search)\n",
    "\n",
    "\n",
    "Stemmer : Algorithms using which we can perform stemming \n",
    "\n",
    "- eg:\n",
    "    - Portor stemmer (for english)\n",
    "    - Snowball Stemmer(for other language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1b95b24a-70a0-46be-aa31-d93ebbaa6065",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# Create a Porter Stemmer object\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def stem_words(text):\n",
    "    # Split the input 'text' into individual words and apply stemming to each word\n",
    "    # Join the stemmed words back into a single string using spaces and return it\n",
    "    return ' '.join([ps.stem(word) for word in text.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c66186bd-6dc8-4701-9e57-b6c2a84e6283",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'walk walk walk walk'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = 'walk walks walking walked'\n",
    "stem_words(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0befce56-9d45-494d-8f92-a9c04327f1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The quick brown fox jumps over the lazy dog. The dogs were barking loudly, but the fox didn't seem to care. It continued to run through the fields, chasing after its prey. The fox's agility and speed were unmatched, making it a formidable hunter in the animal kingdom.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9da29eab-f48d-4f17-b883-cb02e628c40c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"the quick brown fox jump over the lazi dog. the dog were bark loudly, but the fox didn't seem to care. it continu to run through the fields, chase after it prey. the fox' agil and speed were unmatched, make it a formid hunter in the anim kingdom.\""
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem_words(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550e3e4e-1be1-493b-b98b-e3a54475fd5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6beb7ed2-88ec-4f88-aabf-e9b0188092f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f830fabb-6e84-41da-b255-dca71907ca2f",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "\n",
    "Lemmatization, unlike stemming,reduces the inflected words properly ensuring that the root word belongs to the language. In Lemmatization root word is called Lemma. A lemma (plural lemmas or lemmata) is the canonical form, dictionary form, or citation form of a set of words.\n",
    "\n",
    "- almost same as stemming, but the root word here is a valid word\n",
    "- It takes a little longer time when compared to stemming\n",
    "\n",
    "- if we dont have to show the output to the user then we can use stemming\n",
    "- else we can use lemmatization\n",
    "\n",
    "  #### Lemmatization is done using a lexical dictionary instead of an algorithm.\n",
    "  The WORDNET lexical dictionary is used here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8e5587d0-a894-4b3e-bca8-8b14ff746d67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['He', 'wa', 'running', 'and', 'eating', 'at', 'the', 'same', 'time', 'He', 'ha', 'a', 'bad', 'habit', 'of', 'swimming', 'after', 'playing', 'long', 'hour', 'in', 'the', 'Sun']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# Initialize the WordNet Lemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "sentence = '''He was running and eating at the same time. He has a bad habit of swimming after playing \n",
    "long hours in the Sun.'''\n",
    "\n",
    "\n",
    "# Iterate through each word in the sentence\n",
    "for word in sentence_words:\n",
    "    if word not in punctuations:\n",
    "        # If the word is not a punctuation mark, append it to the filtered_words list\n",
    "        filtered_words.append(word)\n",
    "# Lemmatize the words using the WordNet Lemmatizer\n",
    "lemmatized_words = [wordnet_lemmatizer.lemmatize(word) for word in filtered_words]\n",
    "\n",
    "print(lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5fc7a281-d2c0-4ff4-9286-f1559afbe675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Lemma               \n",
      "He                  He                  \n",
      "was                 be                  \n",
      "running             run                 \n",
      "and                 and                 \n",
      "eating              eat                 \n",
      "at                  at                  \n",
      "the                 the                 \n",
      "same                same                \n",
      "time                time                \n",
      ".                   .                   \n",
      "He                  He                  \n",
      "has                 have                \n",
      "a                   a                   \n",
      "bad                 bad                 \n",
      "habit               habit               \n",
      "of                  of                  \n",
      "swimming            swim                \n",
      "after               after               \n",
      "playing             play                \n",
      "long                long                \n",
      "hours               hours               \n",
      "in                  in                  \n",
      "the                 the                 \n",
      "Sun                 Sun                 \n",
      ".                   .                   \n"
     ]
    }
   ],
   "source": [
    "print(\"{0:20}{1:20}\".format(\"Word\",\"Lemma\"))\n",
    "for word in sentence_words:\n",
    "    print('{0:20}{1:20}'.format(word,wordnet_lemmatizer.lemmatize(word,pos='v')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519b8d3d-f0ff-4587-be67-3dbe6b86d51e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cd2430-45ba-4d28-a97e-b74d172e6c9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287fe83e-2b95-4f7f-b512-9b12de5c806c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
